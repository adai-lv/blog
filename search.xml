<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hive 安装和配置</title>
      <link href="/article/hive-an-zhuang-he-pei-zhi.html"/>
      <url>/article/hive-an-zhuang-he-pei-zhi.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>系统环境：MacOS<br>Hadoop 版本：3.1.2</p></blockquote><h2 id="Homebrew安装Hive"><a href="#Homebrew安装Hive" class="headerlink" title="Homebrew安装Hive"></a>Homebrew安装Hive</h2><pre><code>$ brew install hive$ cd /usr/local/Celler/hive/3.1.2/libexec</code></pre><h2 id="环境变量设置"><a href="#环境变量设置" class="headerlink" title="环境变量设置"></a>环境变量设置</h2><pre><code>$ vim ~/.zshrcexport HIVE_HOME="/usr/local/Cellar/hive/3.1.2/libexec"  export PATH="$HIVE_HOME/bin:$PATH"$ source ~/.zshrc</code></pre><h2 id="配置文件设置"><a href="#配置文件设置" class="headerlink" title="配置文件设置"></a>配置文件设置</h2><blockquote><p>在 <strong>libexec/conf</strong> 下提供了一些 <strong>.template</strong> 模板，拷贝文件并去掉 <strong>.template</strong> 后缀即可</p></blockquote><h3 id="修改日志文件"><a href="#修改日志文件" class="headerlink" title="修改日志文件"></a>修改日志文件</h3><pre><code>$ cp hive-log4j2.properties.template hive-log4j2.properties$ cp beeline-log4j2.properties.template beeline-log4j2.properties$ cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties$ cp llap-daemon-log4j2.properties.template llap-daemon-log4j2.properties$ cp llap-cli-log4j2.properties.template llap-cli-log4j2.properties# 更改 hive log 目录，默认为：/tmp$ vim hive-log4j2.propertiesproperty.hive.log.dir = /usr/local/Cellar/hive/3.1.2/libexec/logs</code></pre><h3 id="hive-site-xml-配置"><a href="#hive-site-xml-配置" class="headerlink" title="hive-site.xml 配置"></a>hive-site.xml 配置</h3><blockquote><p>将 <strong>hive-default.xml.template</strong> 文件复制一份，并且改名为 <strong>hive-site.xml</strong></p></blockquote><pre><code>$ cp hive-default.xml.template hive-site.xml</code></pre><ul><li>在 hdfs 创建 hive 目录（在hive-site.xml中有这样的配置）<pre><code>&lt;property&gt;  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;  &lt;description&gt;Hive 默认的数据文件存储路径，通常为 HDFS 可写的路径&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;  &lt;value&gt;/tmp/hive&lt;/value&gt;  &lt;description&gt;HDFS路径，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.cli.print.header&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;</code></pre></li></ul><p>在hdfs中新建目录 <strong>/user/hive/warehouse</strong> 和 <strong>/tmp/hive</strong>，赋予读写权限</p><pre><code>$ hadoop fs -mkdir -p /user/hive/warehouse$ hadoop fs -chmod 777 /user/hive/warehouse$ hadoop fs -mkdir -p /tmp/hive$ hadoop fs -chmod 777 /tmp/hive</code></pre><ul><li><p>修改 hive 临时目录</p><blockquote><p>将 ${system:java.io.tmpdir} 替换为本地hive的临时目录(/usr/local/Cellar/hive/3.1.2/tmp/hive)，并赋予读写权限；<br>将 ${system:user.name} 替换为root；</p></blockquote><pre><code>&lt;property&gt;  &lt;name&gt;hive.querylog.location&lt;/name&gt;  &lt;value&gt;${system:java.io.tmpdir}/${system:user.name}&lt;/value&gt;  &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;  &lt;value&gt;${system:java.io.tmpdir}/${system:user.name}/operation_logs&lt;/value&gt;  &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;  &lt;value&gt;${system:java.io.tmpdir}/${system:user.name}&lt;/value&gt;  &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;  &lt;value&gt;${system:java.io.tmpdir}/${hive.session.id}_resources&lt;/value&gt;  &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;</code></pre></li><li><p>修改数据库相关的配置</p><blockquote><p>javax.jdo.option.ConnectionURL    将对应的value修改为MySQL的地址<br>javax.jdo.option.ConnectionDriverName    将对应的value修改为MySQL驱动类路径<br>javax.jdo.option.ConnectionUserName    将对应的value修改为MySQL数据库登录名<br>javax.jdo.option.ConnectionPassword    将对应的value修改为MySQL数据库的登录密码<br>hive.metastore.schema.verification    将对应的value修改为false</p></blockquote></li></ul><pre><code>&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;    &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;</code></pre><p>下载MySQL驱动包到lib目录<br><a href="https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz" target="_blank" rel="noopener">https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz</a></p><ul><li>WebUI<blockquote><p>Hive从2.0版本开始，为HiveServer2提供了一个简单的WEB UI界面，界面中可以直观的看到当前链接的会话、历史日志、配置参数以及度量信息。</p></blockquote></li></ul><pre><code>&lt;property&gt;    &lt;name&gt;hive.server2.webui.host&lt;/name&gt;    &lt;value&gt;127.0.0.1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hive.server2.webui.port&lt;/name&gt;    &lt;value&gt;10002&lt;/value&gt;&lt;/property&gt;</code></pre><p>需要重启HiveServer2</p><pre><code>$ hive --service hiveserver2 &amp;</code></pre><h3 id="hive-env-sh-配置"><a href="#hive-env-sh-配置" class="headerlink" title="hive-env.sh 配置"></a>hive-env.sh 配置</h3><blockquote><p>将 <strong>hive-env.sh.template</strong> 文件复制一份，并且改名为 <strong>hive-env.sh</strong> 文件</p></blockquote><pre><code>$ cp hive-env.sh.template hive-env.sh$ vim hive-env.shexport HADOOP_HOME=/usr/local/Cellar/hadoop/3.2.1/libexecexport HIVE_CONF_DIR=/usr/local/Cellar/hive/3.1.2/confexport HIVE_AUX_JARS_PATH=/usr/local/Cellar/hive/3.1.2/lib</code></pre><h2 id="启动和测试"><a href="#启动和测试" class="headerlink" title="启动和测试"></a>启动和测试</h2><h3 id="对MySQL数据库进行初始化"><a href="#对MySQL数据库进行初始化" class="headerlink" title="对MySQL数据库进行初始化"></a>对MySQL数据库进行初始化</h3><blockquote><p>执行成功后，hive数据库里已经有一堆表创建好了</p></blockquote><pre><code>$ /usr/local/opt/hive/bin/schematool -initSchema -dbType mysql</code></pre><h3 id="启动hive"><a href="#启动hive" class="headerlink" title="启动hive"></a>启动hive</h3><pre><code>$ /usr/local/opt/hive/bin/hiveor$ hive</code></pre><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><pre><code># 进入hive命令行&gt; show functions;</code></pre><h3 id="新建表以及导入数据的测试"><a href="#新建表以及导入数据的测试" class="headerlink" title="新建表以及导入数据的测试"></a>新建表以及导入数据的测试</h3><pre><code>&gt; create database db_hive_edu;&gt; use db_hive_edu;&gt; create table student(id int,name string) row format delimited fields terminated by '\t';# 将文件数据写入表中$ touch /opt/hive/student.txt001 zhangsan002 lisi003 wangwu004 zhaoliu005 chenqi# 载入表&gt; load data local inpath '/opt/hive/student.txt' into table db_hive_edu.student;# 测试&gt; select * from student;OK001 zhangsan002 lisi003 wangwu004 zhaoliu005 chenqi# 查看hdfs上数据/user/hive/warehouse/db_hive_edu.db/student# 在MySQL中查看$ SELECT * FROM hive.TBLS;</code></pre><h3 id="错误和解决"><a href="#错误和解决" class="headerlink" title="错误和解决"></a>错误和解决</h3><ol><li>WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</li></ol><p>解决方案：实际上其实这个警告可以不予理会。</p><ol start="2"><li>There are 2 datanode(s) running and 2 node(s) areexcluded in this operation.</li></ol><p>发生原因：hadoop中的datanode有问题，没法写入数据。</p><p>解决方案：检查hadoop是否正常运行。</p><ol start="3"><li>Class path contains multiple SLF4J bindings.<pre><code>SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</code></pre></li></ol><p>发生原因：hive 和 hadoop 依赖的 log4j-slf4j 包版本不一致，造成冲突</p><p>解决方案：删除 hive lib 目录下的 log4j-slf4j 包；</p><pre><code>$ rm /usr/local/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar</code></pre><ol start="4"><li>Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument<pre><code>Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357) at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338) at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536) at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554) at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:448) at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5141) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5104) at org.apache.hive.beeline.HiveSchemaTool.&lt;init&gt;(HiveSchemaTool.java:96) at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:323) at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</code></pre></li></ol><p>发生原因：hive内依赖的guava.jar和hadoop内的版本不一致造成的。</p><p>解决方案：</p><ul><li>查看hadoop安装目录下share/hadoop/common/lib内guava.jar版本；</li><li>查看hive安装目录下lib内guava.jar的版本；</li><li>如果两者不一致，删除版本低的，并拷贝高版本的到相应的目录下；<pre><code>$ rm /usr/local/Cellar/hive/3.1.2/libexec/lib/guava-19.0.jar$ cp /usr/local/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/Cellar/hive/3.1.2/libexec/lib/</code></pre></li></ul><ol start="5"><li>com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character<pre><code>Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8at [row,col,system-id]: [3215,96,"file:/usr/local/Cellar/hive/3.1.2/libexec/conf/hive-site.xml"] at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3024) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2973) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848) at org.apache.hadoop.conf.Configuration.get(Configuration.java:1460) at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:4996) at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:5069) at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5156) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5104) at org.apache.hive.beeline.HiveSchemaTool.&lt;init&gt;(HiveSchemaTool.java:96) at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:323) at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</code></pre></li></ol><p>发生原因：hive-site.xml包括无效字符</p><pre><code>&lt;property&gt;    &lt;name&gt;hive.txn.xlock.iow&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;    &lt;description&gt;      Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&amp;...8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently      are not hidden by the INSERT OVERWRITE.    &lt;/description&gt;&lt;/property</code></pre><p>解决方案：去掉无效字符</p><pre><code>&lt;property&gt;    &lt;name&gt;hive.txn.xlock.iow&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;    &lt;description&gt;      Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently      are not hidden by the INSERT OVERWRITE.    &lt;/description&gt;&lt;/property&gt;</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
